{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwGWNHs2xIsx"
   },
   "source": [
    "# Logistic Regression Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcaBeePgu_Tu"
   },
   "source": [
    "## Multi-class classification of MNIST using Logistic Regression\n",
    "\n",
    "The multi-class scenario for logistic regression is quite similar to the binary case, except that the label $y$ is now an integer in {1, ...., K} where $K$ is the number of classes. In this excercise you will be provided with handwritten digit images. Write the code and compute the test accuracy by training a logistic regression based classifier in (i) one-vs-one, and (ii) one-vs-all setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running importer\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        #print('searching: %s'%nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        #print('searching: %s' % nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        #print('Found %d cells'%len(nb.cells))\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "\n",
    "#  register the NotebookFinder with sys.meta_path\n",
    "print('running importer')\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9944,
     "status": "ok",
     "timestamp": 1596983406360,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "ManRVu7IsIjp",
    "outputId": "b48dd937-f2d5-4762-af1a-44fa03c44d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "from utils import plot_decision_boundary, get_accuracy, get_prediction\n",
    "from utils import plot_2D_input_datapoints, generate_gifs, sigmoid, normalize\n",
    "import math\n",
    "import gif\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9918,
     "status": "ok",
     "timestamp": 1596983406361,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "xbV2U06Cs45b"
   },
   "outputs": [],
   "source": [
    "# Let's initialize our weights using uniform distribution\n",
    "def weight_init_uniform_dist(X, y):\n",
    "  \n",
    "    np.random.seed(312)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    _, n_outputs = np.shape(y)\n",
    "\n",
    "    limit = 1 / math.sqrt(n_features)\n",
    "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
    "    weights[-1] = 0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36195,
     "status": "ok",
     "timestamp": 1596983432936,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "SAAbK03fLCR1"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "# One hot encoding of our output label vector y\n",
    "def one_hot(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b\n",
    "\n",
    "# Loading dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "Y = digits.target\n",
    "Y = one_hot(Y)\n",
    "\n",
    "# Absorbing weight b of the hyperplane\n",
    "X = digits.data\n",
    "b_ones = np.ones((len(X), 1))\n",
    "X = np.hstack((X, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36177,
     "status": "ok",
     "timestamp": 1596983432939,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "yzdjTbEYLvPK",
    "outputId": "76ed5c87-3298-433d-cf76-d68026c46342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_orig()\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[10])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36148,
     "status": "ok",
     "timestamp": 1596983432942,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "3CIYTv4x65As",
    "outputId": "d9f59ee0-8392-4ba9-8cb8-11afc1669fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (1308, 65)\n",
      "Validation dataset:  (188, 65)\n",
      "Test dataset:  (301, 65)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, val, and test set.\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.167)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape)\n",
    "print(\"Validation dataset: \", X_val.shape)\n",
    "print(\"Test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36132,
     "status": "ok",
     "timestamp": 1596983432945,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "d3NzkO4s68RX"
   },
   "outputs": [],
   "source": [
    "# Normalizing X_train and absorbing weight b of the hyperplane\n",
    "X_normalized_train = normalize(X_train[:, :64])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_train), 1))\n",
    "X_normalized_train = np.hstack((X_normalized_train, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36096,
     "status": "ok",
     "timestamp": 1596983432947,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "pYrK4fK3iyyk",
    "outputId": "a73d5605-db30-4099-f72e-9cca8e2fe3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308, 65)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_features, n_outputs):\n",
    "    const = 1 / np.sqrt(n_features)\n",
    "    W = np.random.randn(n_features, n_outputs) * const\n",
    "    W[-1, :] = 0\n",
    "    return W\n",
    "\n",
    "def cost_fun(Y, Y_pred):\n",
    "    Y_pred = np.where(Y == 1, Y_pred, 1 - Y_pred)\n",
    "    cost = -1 / Y.shape[0] * np.sum(np.log(Y_pred))\n",
    "    return cost\n",
    "\n",
    "def two_class_data(X_in, Y_in, k1, k2):\n",
    "    idx = np.where((Y_in[:, k1] == 1) | (Y_in[:, k2] == 1))\n",
    "    X = X_in[idx]\n",
    "    Y = Y_in[:, k1:k1+1][idx]\n",
    "    return X, Y\n",
    "\n",
    "def accuracy_onevsone(X, Y, W):\n",
    "    n_classes = int(np.sqrt(W.shape[1]))\n",
    "    y_pred = np.argmax(X @ W, axis = 1) // n_classes\n",
    "    return np.sum(Y[np.arange(Y.shape[0]), y_pred]) / Y.shape[0]\n",
    "\n",
    "def accuracy_onevsall(X, Y, W):\n",
    "    y_pred = np.argmax(X @ W, axis = 1)\n",
    "    return np.sum(Y[np.arange(Y.shape[0]), y_pred]) / Y.shape[0]\n",
    "\n",
    "def train(X, Y, epochs=1000, lr=0.01):\n",
    "    n_samples, n_features = X.shape\n",
    "    W = init_weights(n_features, 1)\n",
    "    for epoch in range(epochs):\n",
    "        Y_pred = sigmoid(X @ W)\n",
    "        gradient = 1 / n_samples * X.T @ (Y - Y_pred)\n",
    "        W += lr * gradient\n",
    "        if epoch % (epochs // 5) == 0:\n",
    "            print(f\"Epoch {epoch}: train loss {cost_fun(Y, Y_pred)}\")\n",
    "    return W\n",
    "\n",
    "def onevsoneClassifier(X_train, Y_train, epochs=1000, lr=0.01):\n",
    "    n_examples, n_features = X_train.shape\n",
    "    _, n_classes = Y_train.shape\n",
    "    \n",
    "    W = []\n",
    "    print(f\"Training one vs one classifier starts\")\n",
    "    for k1 in range(n_classes):\n",
    "        for k2 in range(n_classes):\n",
    "            print(f\"Training {k1} vs {k2} classifier\")\n",
    "            X, Y = two_class_data(X_train, Y_train, k1, k2)\n",
    "            W.append(train(X, Y, epochs, lr).ravel())\n",
    "    print(f\"Training one vs one classifier ends\")\n",
    "    return np.array(W).T\n",
    "\n",
    "def onevsallClassifier(X_train, Y_train, epochs=1000, lr=0.01):\n",
    "    n_examples, n_features = X_train.shape\n",
    "    _, n_classes = Y_train.shape\n",
    "    \n",
    "    W = []\n",
    "    print(f\"Training one vs all classifier starts\")\n",
    "    for k in range(n_classes):\n",
    "        print(f\"{k} vs all classifier starts\")\n",
    "        W.append(train(X_train, Y_train[:,k:k+1], epochs, lr).ravel())\n",
    "        print(f\"{k} vs all classifier ends\")\n",
    "    print(f\"Training one vs all classifier ends\")\n",
    "    return np.array(W).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one vs one classifier starts\n",
      "Training 0 vs 0 classifier\n",
      "Epoch 0: train loss 0.77208347635055\n",
      "Epoch 200: train loss 0.2636196182885959\n",
      "Epoch 400: train loss 0.1440873139678237\n",
      "Epoch 600: train loss 0.09686798374504894\n",
      "Epoch 800: train loss 0.07235874170901654\n",
      "Training 0 vs 1 classifier\n",
      "Epoch 0: train loss 0.6929925372456583\n",
      "Epoch 200: train loss 0.6346421651612956\n",
      "Epoch 400: train loss 0.5851182449664675\n",
      "Epoch 600: train loss 0.5418348489114551\n",
      "Epoch 800: train loss 0.503693526058004\n",
      "Training 0 vs 2 classifier\n",
      "Epoch 0: train loss 0.7131518203812998\n",
      "Epoch 200: train loss 0.6632786209470998\n",
      "Epoch 400: train loss 0.6190473306514719\n",
      "Epoch 600: train loss 0.5793648013599311\n",
      "Epoch 800: train loss 0.5436423363427709\n",
      "Training 0 vs 3 classifier\n",
      "Epoch 0: train loss 0.6766307916666436\n",
      "Epoch 200: train loss 0.6339483492631809\n",
      "Epoch 400: train loss 0.5952249458417612\n",
      "Epoch 600: train loss 0.5600379663489854\n",
      "Epoch 800: train loss 0.5280243182751937\n",
      "Training 0 vs 4 classifier\n",
      "Epoch 0: train loss 0.6681733308714274\n",
      "Epoch 200: train loss 0.6276970398439675\n",
      "Epoch 400: train loss 0.5910116584720452\n",
      "Epoch 600: train loss 0.5576890423461971\n",
      "Epoch 800: train loss 0.5273751824545627\n",
      "Training 0 vs 5 classifier\n",
      "Epoch 0: train loss 0.6489821544173874\n",
      "Epoch 200: train loss 0.6132823665548668\n",
      "Epoch 400: train loss 0.5813290069308974\n",
      "Epoch 600: train loss 0.5521382343123657\n",
      "Epoch 800: train loss 0.5253408353215525\n",
      "Training 0 vs 6 classifier\n",
      "Epoch 0: train loss 0.6984789636306818\n",
      "Epoch 200: train loss 0.658959224587406\n",
      "Epoch 400: train loss 0.6228177044706261\n",
      "Epoch 600: train loss 0.5897158862713724\n",
      "Epoch 800: train loss 0.5593677380787708\n",
      "Training 0 vs 7 classifier\n",
      "Epoch 0: train loss 0.7164422396581571\n",
      "Epoch 200: train loss 0.6575744255096425\n",
      "Epoch 400: train loss 0.6056202151621001\n",
      "Epoch 600: train loss 0.559715842669851\n",
      "Epoch 800: train loss 0.5190837255464854\n",
      "Training 0 vs 8 classifier\n",
      "Epoch 0: train loss 0.6790771408200722\n",
      "Epoch 200: train loss 0.6452231342000029\n",
      "Epoch 400: train loss 0.6141918646777609\n",
      "Epoch 600: train loss 0.5854479647203393\n",
      "Epoch 800: train loss 0.5587584250406284\n",
      "Training 0 vs 9 classifier\n",
      "Epoch 0: train loss 0.7009997256177061\n",
      "Epoch 200: train loss 0.66955803750659\n",
      "Epoch 400: train loss 0.6417247799802553\n",
      "Epoch 600: train loss 0.6158741289728755\n",
      "Epoch 800: train loss 0.5916423244066079\n",
      "Training 1 vs 0 classifier\n",
      "Epoch 0: train loss 0.707522747136538\n",
      "Epoch 200: train loss 0.6496083036124639\n",
      "Epoch 400: train loss 0.5987203391028223\n",
      "Epoch 600: train loss 0.5539416516833527\n",
      "Epoch 800: train loss 0.5144501598462569\n",
      "Training 1 vs 1 classifier\n",
      "Epoch 0: train loss 0.6851073523122616\n",
      "Epoch 200: train loss 0.2597286611716289\n",
      "Epoch 400: train loss 0.14750370699460197\n",
      "Epoch 600: train loss 0.10083832312151259\n",
      "Epoch 800: train loss 0.07600291717699086\n",
      "Training 1 vs 2 classifier\n",
      "Epoch 0: train loss 0.7159727357385869\n",
      "Epoch 200: train loss 0.6872994731659541\n",
      "Epoch 400: train loss 0.6620380104744573\n",
      "Epoch 600: train loss 0.638669015479623\n",
      "Epoch 800: train loss 0.6168316652374009\n",
      "Training 1 vs 3 classifier\n",
      "Epoch 0: train loss 0.7194505642274329\n",
      "Epoch 200: train loss 0.6854507527739516\n",
      "Epoch 400: train loss 0.6550035681972062\n",
      "Epoch 600: train loss 0.6269754989345704\n",
      "Epoch 800: train loss 0.6010219645010445\n",
      "Training 1 vs 4 classifier\n",
      "Epoch 0: train loss 0.7195133440727696\n",
      "Epoch 200: train loss 0.6916564869715582\n",
      "Epoch 400: train loss 0.6662773873598542\n",
      "Epoch 600: train loss 0.6425662907795013\n",
      "Epoch 800: train loss 0.6203013273050818\n",
      "Training 1 vs 5 classifier\n",
      "Epoch 0: train loss 0.6929253429792531\n",
      "Epoch 200: train loss 0.6590959073215187\n",
      "Epoch 400: train loss 0.6278116090327065\n",
      "Epoch 600: train loss 0.5988687852198178\n",
      "Epoch 800: train loss 0.5720751018899434\n",
      "Training 1 vs 6 classifier\n",
      "Epoch 0: train loss 0.7150649599887462\n",
      "Epoch 200: train loss 0.6752168718274091\n",
      "Epoch 400: train loss 0.6389232261030837\n",
      "Epoch 600: train loss 0.6058500528067227\n",
      "Epoch 800: train loss 0.5756842685178735\n",
      "Training 1 vs 7 classifier\n",
      "Epoch 0: train loss 0.6874706231827361\n",
      "Epoch 200: train loss 0.6526483157597198\n",
      "Epoch 400: train loss 0.6246102673455323\n",
      "Epoch 600: train loss 0.5992147711806254\n",
      "Epoch 800: train loss 0.5756224933564886\n",
      "Training 1 vs 8 classifier\n",
      "Epoch 0: train loss 0.6895055733458756\n",
      "Epoch 200: train loss 0.6752087095775541\n",
      "Epoch 400: train loss 0.6617634997560715\n",
      "Epoch 600: train loss 0.6488828930327999\n",
      "Epoch 800: train loss 0.6365007346943918\n",
      "Training 1 vs 9 classifier\n",
      "Epoch 0: train loss 0.6838690215553903\n",
      "Epoch 200: train loss 0.6504591809073105\n",
      "Epoch 400: train loss 0.6207805135546773\n",
      "Epoch 600: train loss 0.5937237410781644\n",
      "Epoch 800: train loss 0.5689069945220476\n",
      "Training 2 vs 0 classifier\n",
      "Epoch 0: train loss 0.6983321374740589\n",
      "Epoch 200: train loss 0.6501388107488177\n",
      "Epoch 400: train loss 0.6072497943818999\n",
      "Epoch 600: train loss 0.5687352809043679\n",
      "Epoch 800: train loss 0.5340458965520105\n",
      "Training 2 vs 1 classifier\n",
      "Epoch 0: train loss 0.6921173267092918\n",
      "Epoch 200: train loss 0.6670483240271574\n",
      "Epoch 400: train loss 0.6437769935511364\n",
      "Epoch 600: train loss 0.6219969712473462\n",
      "Epoch 800: train loss 0.6015688765261837\n",
      "Training 2 vs 2 classifier\n",
      "Epoch 0: train loss 0.6731515483600784\n",
      "Epoch 200: train loss 0.25417075760907315\n",
      "Epoch 400: train loss 0.14430980911631655\n",
      "Epoch 600: train loss 0.09866451852057603\n",
      "Epoch 800: train loss 0.0743727816763258\n",
      "Training 2 vs 3 classifier\n",
      "Epoch 0: train loss 0.6800289862864621\n",
      "Epoch 200: train loss 0.6587840260462102\n",
      "Epoch 400: train loss 0.6390106286163976\n",
      "Epoch 600: train loss 0.6203117920824003\n",
      "Epoch 800: train loss 0.6025729152289618\n",
      "Training 2 vs 4 classifier\n",
      "Epoch 0: train loss 0.7274819577493649\n",
      "Epoch 200: train loss 0.6684113422040527\n",
      "Epoch 400: train loss 0.6172107031249254\n",
      "Epoch 600: train loss 0.5721346686461011\n",
      "Epoch 800: train loss 0.5322558290328475\n",
      "Training 2 vs 5 classifier\n",
      "Epoch 0: train loss 0.6814704886146644\n",
      "Epoch 200: train loss 0.6487287618498332\n",
      "Epoch 400: train loss 0.6185217282510783\n",
      "Epoch 600: train loss 0.5904890040900881\n",
      "Epoch 800: train loss 0.5644322807425693\n",
      "Training 2 vs 6 classifier\n",
      "Epoch 0: train loss 0.6782644659632017\n",
      "Epoch 200: train loss 0.6382384241568982\n",
      "Epoch 400: train loss 0.6018281107137066\n",
      "Epoch 600: train loss 0.5685966891781934\n",
      "Epoch 800: train loss 0.5382217852159121\n",
      "Training 2 vs 7 classifier\n",
      "Epoch 0: train loss 0.6740418395682983\n",
      "Epoch 200: train loss 0.6386565912905882\n",
      "Epoch 400: train loss 0.6063862404541438\n",
      "Epoch 600: train loss 0.5768022850473089\n",
      "Epoch 800: train loss 0.5496250865484695\n",
      "Training 2 vs 8 classifier\n",
      "Epoch 0: train loss 0.6899431636644713\n",
      "Epoch 200: train loss 0.6706256345952344\n",
      "Epoch 400: train loss 0.6522709005484151\n",
      "Epoch 600: train loss 0.6347903423539438\n",
      "Epoch 800: train loss 0.6181316351723402\n",
      "Training 2 vs 9 classifier\n",
      "Epoch 0: train loss 0.7117750208654637\n",
      "Epoch 200: train loss 0.677931753594696\n",
      "Epoch 400: train loss 0.646590513210025\n",
      "Epoch 600: train loss 0.6175536194838492\n",
      "Epoch 800: train loss 0.5906360516794522\n",
      "Training 3 vs 0 classifier\n",
      "Epoch 0: train loss 0.6733480654245727\n",
      "Epoch 200: train loss 0.6311137577090195\n",
      "Epoch 400: train loss 0.5927589020137626\n",
      "Epoch 600: train loss 0.5578963844945295\n",
      "Epoch 800: train loss 0.5261711754063103\n",
      "Training 3 vs 1 classifier\n",
      "Epoch 0: train loss 0.7054051186825216\n",
      "Epoch 200: train loss 0.6736162643147254\n",
      "Epoch 400: train loss 0.6442058722018823\n",
      "Epoch 600: train loss 0.6169565113373824\n",
      "Epoch 800: train loss 0.5916866947132833\n",
      "Training 3 vs 2 classifier\n",
      "Epoch 0: train loss 0.7122444161192005\n",
      "Epoch 200: train loss 0.6881953144259282\n",
      "Epoch 400: train loss 0.6665282787095727\n",
      "Epoch 600: train loss 0.6461783484155217\n",
      "Epoch 800: train loss 0.6269133542970177\n",
      "Training 3 vs 3 classifier\n",
      "Epoch 0: train loss 0.6945570856079233\n",
      "Epoch 200: train loss 0.2563376105411739\n",
      "Epoch 400: train loss 0.14429014134594617\n",
      "Epoch 600: train loss 0.09827169112915637\n",
      "Epoch 800: train loss 0.07392217430154668\n",
      "Training 3 vs 4 classifier\n",
      "Epoch 0: train loss 0.6720829344946077\n",
      "Epoch 200: train loss 0.6129278267229277\n",
      "Epoch 400: train loss 0.5641111771689273\n",
      "Epoch 600: train loss 0.5217574188520117\n",
      "Epoch 800: train loss 0.48452786857667296\n",
      "Training 3 vs 5 classifier\n",
      "Epoch 0: train loss 0.6890510378303121\n",
      "Epoch 200: train loss 0.6609079213024451\n",
      "Epoch 400: train loss 0.6347713278237971\n",
      "Epoch 600: train loss 0.6104317450010983\n",
      "Epoch 800: train loss 0.5877381982580139\n",
      "Training 3 vs 6 classifier\n",
      "Epoch 0: train loss 0.7119190942827799\n",
      "Epoch 200: train loss 0.6530910377938802\n",
      "Epoch 400: train loss 0.6041936824595129\n",
      "Epoch 600: train loss 0.5613399691736101\n",
      "Epoch 800: train loss 0.5233127303698214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3 vs 7 classifier\n",
      "Epoch 0: train loss 0.6874665254864257\n",
      "Epoch 200: train loss 0.6525989846740727\n",
      "Epoch 400: train loss 0.6211831820219906\n",
      "Epoch 600: train loss 0.5923067681573589\n",
      "Epoch 800: train loss 0.5656453633346605\n",
      "Training 3 vs 8 classifier\n",
      "Epoch 0: train loss 0.6951062850498961\n",
      "Epoch 200: train loss 0.6745043846875132\n",
      "Epoch 400: train loss 0.6555277584431067\n",
      "Epoch 600: train loss 0.6375980431798023\n",
      "Epoch 800: train loss 0.6205750598686374\n",
      "Training 3 vs 9 classifier\n",
      "Epoch 0: train loss 0.6935598642453762\n",
      "Epoch 200: train loss 0.6732518988198936\n",
      "Epoch 400: train loss 0.6587100504474483\n",
      "Epoch 600: train loss 0.6455102461816707\n",
      "Epoch 800: train loss 0.632901347285687\n",
      "Training 4 vs 0 classifier\n",
      "Epoch 0: train loss 0.6691257768358464\n",
      "Epoch 200: train loss 0.6288418315949256\n",
      "Epoch 400: train loss 0.5922737505234715\n",
      "Epoch 600: train loss 0.559042483206258\n",
      "Epoch 800: train loss 0.5288031668722173\n",
      "Training 4 vs 1 classifier\n",
      "Epoch 0: train loss 0.7135887721182822\n",
      "Epoch 200: train loss 0.6869431907435316\n",
      "Epoch 400: train loss 0.662039311001825\n",
      "Epoch 600: train loss 0.6386612778819404\n",
      "Epoch 800: train loss 0.6166917400141145\n",
      "Training 4 vs 2 classifier\n",
      "Epoch 0: train loss 0.6984978141149092\n",
      "Epoch 200: train loss 0.6432529213347039\n",
      "Epoch 400: train loss 0.5945533785880779\n",
      "Epoch 600: train loss 0.5515188854026759\n",
      "Epoch 800: train loss 0.5134049250566519\n",
      "Training 4 vs 3 classifier\n",
      "Epoch 0: train loss 0.7116971249654005\n",
      "Epoch 200: train loss 0.6473146510549999\n",
      "Epoch 400: train loss 0.5944563515063509\n",
      "Epoch 600: train loss 0.5486948140722318\n",
      "Epoch 800: train loss 0.5085368458373857\n",
      "Training 4 vs 4 classifier\n",
      "Epoch 0: train loss 0.719369792213016\n",
      "Epoch 200: train loss 0.2632031594584479\n",
      "Epoch 400: train loss 0.14734606144832282\n",
      "Epoch 600: train loss 0.10006698772435368\n",
      "Epoch 800: train loss 0.07514670748231864\n",
      "Training 4 vs 5 classifier\n",
      "Epoch 0: train loss 0.7182390035638462\n",
      "Epoch 200: train loss 0.6744235994071421\n",
      "Epoch 400: train loss 0.635446055826272\n",
      "Epoch 600: train loss 0.6000922201739176\n",
      "Epoch 800: train loss 0.5678774726860621\n",
      "Training 4 vs 6 classifier\n",
      "Epoch 0: train loss 0.730176208958132\n",
      "Epoch 200: train loss 0.6834432862590911\n",
      "Epoch 400: train loss 0.6515466996893725\n",
      "Epoch 600: train loss 0.6237666048871197\n",
      "Epoch 800: train loss 0.5981352345140013\n",
      "Training 4 vs 7 classifier\n",
      "Epoch 0: train loss 0.7126359285229392\n",
      "Epoch 200: train loss 0.6756340784784243\n",
      "Epoch 400: train loss 0.6418935690220554\n",
      "Epoch 600: train loss 0.6110172570991079\n",
      "Epoch 800: train loss 0.5827216693108856\n",
      "Training 4 vs 8 classifier\n",
      "Epoch 0: train loss 0.708501280930647\n",
      "Epoch 200: train loss 0.672950470873747\n",
      "Epoch 400: train loss 0.6434771783807671\n",
      "Epoch 600: train loss 0.6166731253803843\n",
      "Epoch 800: train loss 0.5918243187374603\n",
      "Training 4 vs 9 classifier\n",
      "Epoch 0: train loss 0.6939333770976727\n",
      "Epoch 200: train loss 0.6449007618614099\n",
      "Epoch 400: train loss 0.6015086823826151\n",
      "Epoch 600: train loss 0.5628416292234075\n",
      "Epoch 800: train loss 0.5282845974351471\n",
      "Training 5 vs 0 classifier\n",
      "Epoch 0: train loss 0.7040643811940138\n",
      "Epoch 200: train loss 0.6645852786085962\n",
      "Epoch 400: train loss 0.628677373451209\n",
      "Epoch 600: train loss 0.5958008746059823\n",
      "Epoch 800: train loss 0.5656393761784374\n",
      "Training 5 vs 1 classifier\n",
      "Epoch 0: train loss 0.7235469483633161\n",
      "Epoch 200: train loss 0.6867147761919062\n",
      "Epoch 400: train loss 0.653353509821672\n",
      "Epoch 600: train loss 0.6226207010274315\n",
      "Epoch 800: train loss 0.5942022729311658\n",
      "Training 5 vs 2 classifier\n",
      "Epoch 0: train loss 0.6731959449928254\n",
      "Epoch 200: train loss 0.6412433082525921\n",
      "Epoch 400: train loss 0.6115824584968483\n",
      "Epoch 600: train loss 0.5840227077471196\n",
      "Epoch 800: train loss 0.5583972267673842\n",
      "Training 5 vs 3 classifier\n",
      "Epoch 0: train loss 0.6741932541215394\n",
      "Epoch 200: train loss 0.646636278926109\n",
      "Epoch 400: train loss 0.6213831905693601\n",
      "Epoch 600: train loss 0.5979204195515433\n",
      "Epoch 800: train loss 0.5760484220433562\n",
      "Training 5 vs 4 classifier\n",
      "Epoch 0: train loss 0.7104469635436333\n",
      "Epoch 200: train loss 0.6665835566530076\n",
      "Epoch 400: train loss 0.628298250377287\n",
      "Epoch 600: train loss 0.5936953647951969\n",
      "Epoch 800: train loss 0.5621733018029312\n",
      "Training 5 vs 5 classifier\n",
      "Epoch 0: train loss 0.6844632201205663\n",
      "Epoch 200: train loss 0.2563377213489853\n",
      "Epoch 400: train loss 0.1449983605147542\n",
      "Epoch 600: train loss 0.09895229526858276\n",
      "Epoch 800: train loss 0.07450909691715336\n",
      "Training 5 vs 6 classifier\n",
      "Epoch 0: train loss 0.6975064816551643\n",
      "Epoch 200: train loss 0.654267472500365\n",
      "Epoch 400: train loss 0.6160703510633825\n",
      "Epoch 600: train loss 0.581488272380983\n",
      "Epoch 800: train loss 0.5500022631179479\n",
      "Training 5 vs 7 classifier\n",
      "Epoch 0: train loss 0.6715172018796911\n",
      "Epoch 200: train loss 0.6380372185030166\n",
      "Epoch 400: train loss 0.6090105297950681\n",
      "Epoch 600: train loss 0.5824136289065188\n",
      "Epoch 800: train loss 0.5577547944391763\n",
      "Training 5 vs 8 classifier\n",
      "Epoch 0: train loss 0.721370822650179\n",
      "Epoch 200: train loss 0.6964803577810074\n",
      "Epoch 400: train loss 0.6755038998667816\n",
      "Epoch 600: train loss 0.6559739148560286\n",
      "Epoch 800: train loss 0.6374317004276203\n",
      "Training 5 vs 9 classifier\n",
      "Epoch 0: train loss 0.7127537045222403\n",
      "Epoch 200: train loss 0.6824498998726369\n",
      "Epoch 400: train loss 0.6559514490828571\n",
      "Epoch 600: train loss 0.6316467186648705\n",
      "Epoch 800: train loss 0.6091290104362744\n",
      "Training 6 vs 0 classifier\n",
      "Epoch 0: train loss 0.6864814202548466\n",
      "Epoch 200: train loss 0.6418012327778454\n",
      "Epoch 400: train loss 0.6062140623131734\n",
      "Epoch 600: train loss 0.5744748105501439\n",
      "Epoch 800: train loss 0.5454987225724256\n",
      "Training 6 vs 1 classifier\n",
      "Epoch 0: train loss 0.694176056289899\n",
      "Epoch 200: train loss 0.6515132463424052\n",
      "Epoch 400: train loss 0.6163346634162165\n",
      "Epoch 600: train loss 0.5849046246523532\n",
      "Epoch 800: train loss 0.5563289358892235\n",
      "Training 6 vs 2 classifier\n",
      "Epoch 0: train loss 0.7232206394461785\n",
      "Epoch 200: train loss 0.6794477307045919\n",
      "Epoch 400: train loss 0.6395145041193282\n",
      "Epoch 600: train loss 0.6030698141018448\n",
      "Epoch 800: train loss 0.5697858113700703\n",
      "Training 6 vs 3 classifier\n",
      "Epoch 0: train loss 0.6824208830266141\n",
      "Epoch 200: train loss 0.6242538400102108\n",
      "Epoch 400: train loss 0.5781869966771657\n",
      "Epoch 600: train loss 0.5382021167974953\n",
      "Epoch 800: train loss 0.5027469136359972\n",
      "Training 6 vs 4 classifier\n",
      "Epoch 0: train loss 0.6865184396486006\n",
      "Epoch 200: train loss 0.6523922867387983\n",
      "Epoch 400: train loss 0.6242387074181606\n",
      "Epoch 600: train loss 0.5985509502495299\n",
      "Epoch 800: train loss 0.574629026533576\n",
      "Training 6 vs 5 classifier\n",
      "Epoch 0: train loss 0.6765614740495479\n",
      "Epoch 200: train loss 0.6365099390876117\n",
      "Epoch 400: train loss 0.6000875774991797\n",
      "Epoch 600: train loss 0.566917522299061\n",
      "Epoch 800: train loss 0.5366717800832621\n",
      "Training 6 vs 6 classifier\n",
      "Epoch 0: train loss 0.6457764204703801\n",
      "Epoch 200: train loss 0.24223541580459929\n",
      "Epoch 400: train loss 0.1377751435394163\n",
      "Epoch 600: train loss 0.0943562458379977\n",
      "Epoch 800: train loss 0.0712129827811666\n",
      "Training 6 vs 7 classifier\n",
      "Epoch 0: train loss 0.7076128498947405\n",
      "Epoch 200: train loss 0.6459708886963959\n",
      "Epoch 400: train loss 0.592997562128358\n",
      "Epoch 600: train loss 0.5466046247335895\n",
      "Epoch 800: train loss 0.505736712758431\n",
      "Training 6 vs 8 classifier\n",
      "Epoch 0: train loss 0.7027859690666555\n",
      "Epoch 200: train loss 0.6721990196975033\n",
      "Epoch 400: train loss 0.6436830077491181\n",
      "Epoch 600: train loss 0.6170902132580811\n",
      "Epoch 800: train loss 0.5922796650998969\n",
      "Training 6 vs 9 classifier\n",
      "Epoch 0: train loss 0.7125538366405698\n",
      "Epoch 200: train loss 0.6580012026472702\n",
      "Epoch 400: train loss 0.6118883288023976\n",
      "Epoch 600: train loss 0.5710413096529353\n",
      "Epoch 800: train loss 0.534463484090302\n",
      "Training 7 vs 0 classifier\n",
      "Epoch 0: train loss 0.6592679802063196\n",
      "Epoch 200: train loss 0.6068544995502053\n",
      "Epoch 400: train loss 0.5606037138226749\n",
      "Epoch 600: train loss 0.5196826133479681\n",
      "Epoch 800: train loss 0.4833871118364442\n",
      "Training 7 vs 1 classifier\n",
      "Epoch 0: train loss 0.6861103726711306\n",
      "Epoch 200: train loss 0.6532883268972651\n",
      "Epoch 400: train loss 0.6254353826843985\n",
      "Epoch 600: train loss 0.5999185477936959\n",
      "Epoch 800: train loss 0.5761655523234382\n",
      "Training 7 vs 2 classifier\n",
      "Epoch 0: train loss 0.6865017098217299\n",
      "Epoch 200: train loss 0.6497561830778926\n",
      "Epoch 400: train loss 0.6165000600814972\n",
      "Epoch 600: train loss 0.5860705426245169\n",
      "Epoch 800: train loss 0.5581404743042656\n",
      "Training 7 vs 3 classifier\n",
      "Epoch 0: train loss 0.6819935997320219\n",
      "Epoch 200: train loss 0.6482355777050138\n",
      "Epoch 400: train loss 0.617404222036828\n",
      "Epoch 600: train loss 0.5889811932777583\n",
      "Epoch 800: train loss 0.5627139633253766\n",
      "Training 7 vs 4 classifier\n",
      "Epoch 0: train loss 0.6625303774481736\n",
      "Epoch 200: train loss 0.6283974675674732\n",
      "Epoch 400: train loss 0.5982041475263137\n",
      "Epoch 600: train loss 0.5707161373441144\n",
      "Epoch 800: train loss 0.5455203215864745\n",
      "Training 7 vs 5 classifier\n",
      "Epoch 0: train loss 0.6864107369373125\n",
      "Epoch 200: train loss 0.6535075433276594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: train loss 0.6237321883329093\n",
      "Epoch 600: train loss 0.5962174000541517\n",
      "Epoch 800: train loss 0.5706739325443267\n",
      "Training 7 vs 6 classifier\n",
      "Epoch 0: train loss 0.6754937535452329\n",
      "Epoch 200: train loss 0.6189754177941981\n",
      "Epoch 400: train loss 0.5697147408099689\n",
      "Epoch 600: train loss 0.5263901566440178\n",
      "Epoch 800: train loss 0.48813882113181384\n",
      "Training 7 vs 7 classifier\n",
      "Epoch 0: train loss 0.7379492078362175\n",
      "Epoch 200: train loss 0.268173743784351\n",
      "Epoch 400: train loss 0.14953764944181117\n",
      "Epoch 600: train loss 0.10134998028324506\n",
      "Epoch 800: train loss 0.07602004177757449\n",
      "Training 7 vs 8 classifier\n",
      "Epoch 0: train loss 0.6755302072366178\n",
      "Epoch 200: train loss 0.6531649138693749\n",
      "Epoch 400: train loss 0.6330134271118832\n",
      "Epoch 600: train loss 0.6140490526769031\n",
      "Epoch 800: train loss 0.5960511539611817\n",
      "Training 7 vs 9 classifier\n",
      "Epoch 0: train loss 0.7087356222233651\n",
      "Epoch 200: train loss 0.6626424567522412\n",
      "Epoch 400: train loss 0.6275454550037378\n",
      "Epoch 600: train loss 0.5966269433291649\n",
      "Epoch 800: train loss 0.5684768632475037\n",
      "Training 8 vs 0 classifier\n",
      "Epoch 0: train loss 0.67505699003906\n",
      "Epoch 200: train loss 0.6415587916043352\n",
      "Epoch 400: train loss 0.6107898203791207\n",
      "Epoch 600: train loss 0.58227729061994\n",
      "Epoch 800: train loss 0.5557996548061226\n",
      "Training 8 vs 1 classifier\n",
      "Epoch 0: train loss 0.7046359466159448\n",
      "Epoch 200: train loss 0.6886304761043919\n",
      "Epoch 400: train loss 0.6744527972966946\n",
      "Epoch 600: train loss 0.6610239152553589\n",
      "Epoch 800: train loss 0.6481424333979033\n",
      "Training 8 vs 2 classifier\n",
      "Epoch 0: train loss 0.7203977678715019\n",
      "Epoch 200: train loss 0.6965063994097793\n",
      "Epoch 400: train loss 0.6766289159732151\n",
      "Epoch 600: train loss 0.658178726526541\n",
      "Epoch 800: train loss 0.6406760955086748\n",
      "Training 8 vs 3 classifier\n",
      "Epoch 0: train loss 0.6788716894000236\n",
      "Epoch 200: train loss 0.6594866422675573\n",
      "Epoch 400: train loss 0.6411969349875761\n",
      "Epoch 600: train loss 0.623843113055155\n",
      "Epoch 800: train loss 0.6073562713013971\n",
      "Training 8 vs 4 classifier\n",
      "Epoch 0: train loss 0.6988507586045346\n",
      "Epoch 200: train loss 0.6649040206208323\n",
      "Epoch 400: train loss 0.6361371206482199\n",
      "Epoch 600: train loss 0.6098540579297702\n",
      "Epoch 800: train loss 0.5854665322965941\n",
      "Training 8 vs 5 classifier\n",
      "Epoch 0: train loss 0.7307317866687542\n",
      "Epoch 200: train loss 0.7084807746878476\n",
      "Epoch 400: train loss 0.6874611033979942\n",
      "Epoch 600: train loss 0.6674553332946247\n",
      "Epoch 800: train loss 0.6483874227231142\n",
      "Training 8 vs 6 classifier\n",
      "Epoch 0: train loss 0.7050905173219806\n",
      "Epoch 200: train loss 0.6734242059862497\n",
      "Epoch 400: train loss 0.6446480203949313\n",
      "Epoch 600: train loss 0.6179376552244287\n",
      "Epoch 800: train loss 0.5930386856336801\n",
      "Training 8 vs 7 classifier\n",
      "Epoch 0: train loss 0.7232902609368858\n",
      "Epoch 200: train loss 0.6987850206872566\n",
      "Epoch 400: train loss 0.6765096821889074\n",
      "Epoch 600: train loss 0.6555178524050973\n",
      "Epoch 800: train loss 0.6355986780343692\n",
      "Training 8 vs 8 classifier\n",
      "Epoch 0: train loss 0.7028285289354185\n",
      "Epoch 200: train loss 0.25952160384057366\n",
      "Epoch 400: train loss 0.1459059171341956\n",
      "Epoch 600: train loss 0.09928509320027924\n",
      "Epoch 800: train loss 0.07463916356341142\n",
      "Training 8 vs 9 classifier\n",
      "Epoch 0: train loss 0.6863007160075815\n",
      "Epoch 200: train loss 0.6676558559233865\n",
      "Epoch 400: train loss 0.6503420428977599\n",
      "Epoch 600: train loss 0.6339429010806009\n",
      "Epoch 800: train loss 0.6183505500900522\n",
      "Training 9 vs 0 classifier\n",
      "Epoch 0: train loss 0.6762924612284201\n",
      "Epoch 200: train loss 0.6484539955869938\n",
      "Epoch 400: train loss 0.6223274463004743\n",
      "Epoch 600: train loss 0.5977921077523962\n",
      "Epoch 800: train loss 0.5747405774680128\n",
      "Training 9 vs 1 classifier\n",
      "Epoch 0: train loss 0.7191453512924201\n",
      "Epoch 200: train loss 0.6832043663696138\n",
      "Epoch 400: train loss 0.6508028923534844\n",
      "Epoch 600: train loss 0.6212046399219693\n",
      "Epoch 800: train loss 0.5940772347604458\n",
      "Training 9 vs 2 classifier\n",
      "Epoch 0: train loss 0.6677211166944893\n",
      "Epoch 200: train loss 0.6370618034694492\n",
      "Epoch 400: train loss 0.6086609907720053\n",
      "Epoch 600: train loss 0.5823343569169503\n",
      "Epoch 800: train loss 0.5579108650902977\n",
      "Training 9 vs 3 classifier\n",
      "Epoch 0: train loss 0.7150422790340066\n",
      "Epoch 200: train loss 0.6988828341570277\n",
      "Epoch 400: train loss 0.6843576413543592\n",
      "Epoch 600: train loss 0.6705094614649472\n",
      "Epoch 800: train loss 0.6571641649740431\n",
      "Training 9 vs 4 classifier\n",
      "Epoch 0: train loss 0.7165120572085433\n",
      "Epoch 200: train loss 0.665319433296143\n",
      "Epoch 400: train loss 0.6198784011389334\n",
      "Epoch 600: train loss 0.5793814762572707\n",
      "Epoch 800: train loss 0.5432122795603492\n",
      "Training 9 vs 5 classifier\n",
      "Epoch 0: train loss 0.7147230545156833\n",
      "Epoch 200: train loss 0.6863155710759153\n",
      "Epoch 400: train loss 0.6601063124553328\n",
      "Epoch 600: train loss 0.6358087908567077\n",
      "Epoch 800: train loss 0.6132492191474561\n",
      "Training 9 vs 6 classifier\n",
      "Epoch 0: train loss 0.6982126639507351\n",
      "Epoch 200: train loss 0.6476708053345761\n",
      "Epoch 400: train loss 0.6033097618393425\n",
      "Epoch 600: train loss 0.5636921931439639\n",
      "Epoch 800: train loss 0.5281400486550692\n",
      "Training 9 vs 7 classifier\n",
      "Epoch 0: train loss 0.6759256815809992\n",
      "Epoch 200: train loss 0.6338650037441296\n",
      "Epoch 400: train loss 0.6013220526728521\n",
      "Epoch 600: train loss 0.5725300367827514\n",
      "Epoch 800: train loss 0.5462786648602626\n",
      "Training 9 vs 8 classifier\n",
      "Epoch 0: train loss 0.6740787777794811\n",
      "Epoch 200: train loss 0.6556304550073204\n",
      "Epoch 400: train loss 0.6387767959509052\n",
      "Epoch 600: train loss 0.6228593179571195\n",
      "Epoch 800: train loss 0.6077302650632191\n",
      "Training 9 vs 9 classifier\n",
      "Epoch 0: train loss 0.7335125788822682\n",
      "Epoch 200: train loss 0.26637786910886774\n",
      "Epoch 400: train loss 0.1485948019539568\n",
      "Epoch 600: train loss 0.10074440109329463\n",
      "Epoch 800: train loss 0.07558387652663323\n",
      "Training one vs one classifier ends\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_onevsone = onevsoneClassifier(X_normalized_train, Y_train)\n",
    "W_onevsone.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one vs all classifier starts\n",
      "0 vs all classifier starts\n",
      "Epoch 0: train loss 1.664570350040058\n",
      "Epoch 200: train loss 0.012751109634733127\n",
      "Epoch 400: train loss 0.007167189652143117\n",
      "Epoch 600: train loss 0.005139368380181322\n",
      "Epoch 800: train loss 0.00408826344654866\n",
      "0 vs all classifier ends\n",
      "1 vs all classifier starts\n",
      "Epoch 0: train loss 2.29764698407135\n",
      "Epoch 200: train loss 0.067179540840804\n",
      "Epoch 400: train loss 0.056895696498393704\n",
      "Epoch 600: train loss 0.05187500972137221\n",
      "Epoch 800: train loss 0.04867523220692286\n",
      "1 vs all classifier ends\n",
      "2 vs all classifier starts\n",
      "Epoch 0: train loss 2.625146099909089\n",
      "Epoch 200: train loss 0.027874662914575188\n",
      "Epoch 400: train loss 0.01660497012389972\n",
      "Epoch 600: train loss 0.012075848336271124\n",
      "Epoch 800: train loss 0.009582452444725924\n",
      "2 vs all classifier ends\n",
      "3 vs all classifier starts\n",
      "Epoch 0: train loss 1.3264571479382945\n",
      "Epoch 200: train loss 0.05698494582767506\n",
      "Epoch 400: train loss 0.041583529313282505\n",
      "Epoch 600: train loss 0.03513461746767037\n",
      "Epoch 800: train loss 0.03157187744450594\n",
      "3 vs all classifier ends\n",
      "4 vs all classifier starts\n",
      "Epoch 0: train loss 4.144098825465042\n",
      "Epoch 200: train loss 0.020644033023261874\n",
      "Epoch 400: train loss 0.014456427380758023\n",
      "Epoch 600: train loss 0.011583806203663312\n",
      "Epoch 800: train loss 0.009819299921122139\n",
      "4 vs all classifier ends\n",
      "5 vs all classifier starts\n",
      "Epoch 0: train loss 11.667555330365538\n",
      "Epoch 200: train loss 0.02640768171012352\n",
      "Epoch 400: train loss 0.017990246164434127\n",
      "Epoch 600: train loss 0.01429236581553648\n",
      "Epoch 800: train loss 0.012082489641175495\n",
      "5 vs all classifier ends\n",
      "6 vs all classifier starts\n",
      "Epoch 0: train loss 4.874552970396107\n",
      "Epoch 200: train loss 0.023992763321476658\n",
      "Epoch 400: train loss 0.015986342276358048\n",
      "Epoch 600: train loss 0.012662088752560488\n",
      "Epoch 800: train loss 0.010723747463854013\n",
      "6 vs all classifier ends\n",
      "7 vs all classifier starts\n",
      "Epoch 0: train loss 1.0852939372621535\n",
      "Epoch 200: train loss 0.029321447231850508\n",
      "Epoch 400: train loss 0.01869966725784806\n",
      "Epoch 600: train loss 0.014754849988906296\n",
      "Epoch 800: train loss 0.01260049167126755\n",
      "7 vs all classifier ends\n",
      "8 vs all classifier starts\n",
      "Epoch 0: train loss 7.6441208514304\n",
      "Epoch 200: train loss 0.12714600441984994\n",
      "Epoch 400: train loss 0.10668982348729707\n",
      "Epoch 600: train loss 0.09953206762107332\n",
      "Epoch 800: train loss 0.09572283331022303\n",
      "8 vs all classifier ends\n",
      "9 vs all classifier starts\n",
      "Epoch 0: train loss 1.5960434845339724\n",
      "Epoch 200: train loss 0.0973811570346086\n",
      "Epoch 400: train loss 0.06291467572553042\n",
      "Epoch 600: train loss 0.051318005857755006\n",
      "Epoch 800: train loss 0.04560897352566035\n",
      "9 vs all classifier ends\n",
      "Training one vs all classifier ends\n"
     ]
    }
   ],
   "source": [
    "W_onevsall = onevsallClassifier(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7522935779816514\n",
      "Validation accuracy: 0.7925531914893617\n",
      "Test accuracy: 0.7641196013289037\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_onevsone(X_train, Y_train, W_onevsone)}\")\n",
    "print(f\"Validation accuracy: {accuracy_onevsone(X_val, Y_val, W_onevsone)}\")\n",
    "print(f\"Test accuracy: {accuracy_onevsone(X_test, Y_test, W_onevsone)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs All Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.981651376146789\n",
      "Validation accuracy: 0.9521276595744681\n",
      "Test accuracy: 0.9568106312292359\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_onevsall(X_train, Y_train, W_onevsall)}\")\n",
    "print(f\"Validation accuracy: {accuracy_onevsall(X_val, Y_val, W_onevsall)}\")\n",
    "print(f\"Test accuracy: {accuracy_onevsall(X_test, Y_test, W_onevsall)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LogisticRegression_draft4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (cvit)",
   "language": "python",
   "name": "cvit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
